import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics, tree , linear_model
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

df = pd.read_csv('C:/Users/alek2/Documents/GitHub/DataScience/data/manufacturing_Task_01.csv')

# Exclude 'FluxCompensation' and 'ionizationclass'
X = df.drop(['id','weight_in_kg', 'weight_in_g', 'error', 'error_type', 'multideminsionality', 'Quality', 'reflectionScore',
              'distortion', 'nicesness', 'multideminsionality', 'ionizationclass', 'FluxCompensation'], axis=1)
y = df['nicesness']

# Check for missing values in 'quality'
print(df['nicesness'].isnull().sum())

# Remove rows with missing or non-numeric values in 'quality'
df_quality = df.dropna(subset=['nicesness'])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size = .50)

lm = DecisionTreeRegressor(random_state=42)
model = lm.fit(X_train, y_train)
predictions = lm.predict(X_test)
score = lm.score(X_test, y_test)
print(score)
plt.scatter(y_test, predictions)
plt.axline([0, 0], [1, 1], dashes=[6, 2], c='red', alpha=0.5)
plt.xlabel("True Values")
plt.ylabel("Predictions")
plt.show()
mae = metrics.mean_absolute_error(y_test, predictions)
mse = metrics.mean_squared_error(y_test, predictions)
print('MAE is {}'.format(mae))
print('MSE is {}'.format(mse))

# Extract feature names from X_train.columns
features = X_train.columns

# Create a DataFrame with feature importance
feature_importance = pd.DataFrame({'Feature': features, 'Importance': lm.feature_importances_})
print(feature_importance)


# RESULTS (check for plots):
# cannot be predicted with any model
# 0
# 1.0
# MAE is 2.280214884692432
# MSE is 8.207535853212379
#       Feature    Importance
# 0       width  1.000000e+00
# 1      height  7.178433e-18
# 2    pressure  6.299234e-18
# 3       karma  5.919490e-18
# 4  modulation  7.190332e-18